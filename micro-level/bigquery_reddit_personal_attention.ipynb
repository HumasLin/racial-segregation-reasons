{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import nltk.data\n",
    "import warnings\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from psaw import PushshiftAPI\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "!pip install google-colab\n",
    "from google.colab import auth\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "auth.authenticate_user()\n",
    "print('Authenticated')\n",
    "client = bigquery.Client(project='socialseg')\n",
    "\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=[]\n",
    "for i in range(2010,2015):\n",
    "    dates.append(i)\n",
    "for i in range(1,13):\n",
    "    for year in range(5,9):\n",
    "        month = '0{}'.format(i)[-2:]\n",
    "        dates.append('201{}_{}'.format(year, month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape raw data from Reddit with Bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_body = []\n",
    "comments_author = []\n",
    "subreddits = []\n",
    "\n",
    "for date in dates:\n",
    "    print(date,len(comments_body))\n",
    "    query = \"\"\"\n",
    "            SELECT body,author,subreddit \n",
    "            FROM `fh-bigquery.reddit_comments.{}` \n",
    "            WHERE body LIKE \"% live %\" and author != '[deleted]'\n",
    "            \"\"\".format(date)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    i = 0\n",
    "    for row in query_job:\n",
    "        comments_body.append(row.body)\n",
    "        comments_author.append(row.author)\n",
    "        subreddits.append(row.subreddit)\n",
    "        i+=1\n",
    "        if i>5500:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.DataFrame({'body':comments_body,'author':comments_author,'subreddit':subreddits}).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find similar comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "def get_consine_sim(*strs):\n",
    "    vectors = [t for t in get_vectors(*strs)]\n",
    "    return cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Examples.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "verified = [line.strip() for line in lines if line!='\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_comments = []\n",
    "authors = []\n",
    "for i in tqdm_notebook(range(len(database['author']))):\n",
    "    data_one = database.iloc[i]\n",
    "    author = data_one['author']\n",
    "    body = data_one['body']\n",
    "    raw_sentences = tokenizer.tokenize(body.strip())\n",
    "\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence.split())<5:\n",
    "            continue\n",
    "        if re.sub(\"[^a-zA-Z]\", \"\", raw_sentence)==\"\":\n",
    "            continue\n",
    "        score = max([get_consine_sim(raw_sentence,v)[1,0] for v in verified])\n",
    "        if score>0.075:\n",
    "            valid_comments.append(raw_sentence)\n",
    "            authors.append(author)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.DataFrame({'body':valid_comments,'author':authors})\n",
    "df_valid.to_csv('valid_sentences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label authors' races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_authors = []\n",
    "\n",
    "for date in dates:\n",
    "    query = \"\"\"\n",
    "            SELECT body,author \n",
    "            FROM `fh-bigquery.reddit_comments.{}` \n",
    "            WHERE (body LIKE \"%I\\'m black %\" OR body LIKE \"%I am black %\" OR body LIKE \"%As a black %\"\n",
    "                   OR body LIKE \"%I\\'m a black %\" OR body LIKE \"%I am a black %\"))\n",
    "            \"\"\".format(date)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    for row in query_job:\n",
    "        black_authors.append(row.author)\n",
    "\n",
    "white_authors = []\n",
    "\n",
    "for date in dates:\n",
    "    query = \"\"\"\n",
    "            SELECT body,author \n",
    "            FROM `fh-bigquery.reddit_comments.{}` \n",
    "            WHERE (body LIKE \"%I\\'m white %\" OR body LIKE \"%I am white %\" OR body LIKE \"%As a white %\"\n",
    "                   OR body LIKE \"%I\\'m a white %\" OR body LIKE \"%I am a white %\")\n",
    "            \"\"\".format(date)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    for row in query_job:\n",
    "        white_authors.append(row.author)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_authors = set(white_authors)\n",
    "black_authors = set(black_authors)\n",
    "invalid_class = white_authors.intersection(black_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_race = []\n",
    "for i in range(len(df_valid)):\n",
    "    author = df_valid.iloc[i]['authors']\n",
    "    if author == '[deleted]':\n",
    "        author_race.append(None)\n",
    "        continue\n",
    "    if author in white_authors and author not in invalid_class:\n",
    "        author_race.append(\"white\")\n",
    "    elif author in black_authors and author not in invalid_class:\n",
    "        author_race.append(\"black\")\n",
    "    else:\n",
    "        author_race.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['race']=author_race\n",
    "df_valid = df_valid.dropna()\n",
    "df_valid = df_valid[['body','race']].reset_index().drop(columns=['index'])\n",
    "df_valid.to_csv('data/complete_data_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
