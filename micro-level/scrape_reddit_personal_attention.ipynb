{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/anaconda3/lib/python3.7/site-packages (7.2.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/lib/python3.7/site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: prawcore<3,>=2 in /opt/anaconda3/lib/python3.7/site-packages (from praw) (2.0.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /opt/anaconda3/lib/python3.7/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/anaconda3/lib/python3.7/site-packages (from prawcore<3,>=2->praw) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2->praw) (2020.6.20)\n",
      "Requirement already satisfied: psaw in /opt/anaconda3/lib/python3.7/site-packages (0.1.0)\n",
      "Requirement already satisfied: Click in /opt/anaconda3/lib/python3.7/site-packages (from psaw) (7.1.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from psaw) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->psaw) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->psaw) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->psaw) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->psaw) (1.24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/humas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/humas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install praw\n",
    "!pip install psaw\n",
    "import re\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm_notebook\n",
    "nltk.download('stopwords')\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from psaw import PushshiftAPI\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "api = PushshiftAPI()\n",
    "reddit = praw.Reddit(client_id='niNh_Elui1OusA', client_secret='gSMcp4CW3nHLolSLmGkiI6Dq_fw', user_agent='Scraping')\n",
    "# reddit = praw.Reddit(client_id='client_id', client_secret='client_secret', user_agent='user_agent')\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "\n",
    "    no_cite = re.sub('>.*','',text)\n",
    "\n",
    "    no_url = re.sub(r\"http\\S+\", '', no_cite)\n",
    "\n",
    "    review_text = BeautifulSoup(no_url).get_text()\n",
    "\n",
    "    #Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    #Convert to lower case and split\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    #Covert stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    #Remove stopwords\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    #Join the words back into one string separated by space\n",
    "    clean_text = \" \".join(meaningful_words)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_vectors(*strs):\n",
    "    text = [clean(t) for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "def get_consine_sim(*strs):\n",
    "    vectors = [t for t in get_vectors(*strs)]\n",
    "    return cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape raw data from Reddit with PRAW and PSAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Examples.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "verified = [line.strip() for line in lines if line!='\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62b1ce9bd8a4b7a973b86798f1b2647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen1 = api.search_comments(q=' live ',subreddit='Atlanta')\n",
    "\n",
    "comments = []\n",
    "comments_body = []\n",
    "comments_author = []\n",
    "\n",
    "for i in tqdm_notebook(gen1):\n",
    "\n",
    "    c_id = str(i)\n",
    "    comment_entity=reddit.comment(c_id)\n",
    "    c_body = comment_entity.body\n",
    "    c_author = str(comment_entity.author)\n",
    "    verified_rest = list(set(verified) - set([i]))\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(c_body.strip())\n",
    "\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence.split())<5:\n",
    "            continue\n",
    "        if re.sub(\"[^a-zA-Z]\", \"\", raw_sentence)==\"\":\n",
    "            continue\n",
    "        score = max([get_consine_sim(raw_sentence,v)[1,0] for v in verified])\n",
    "        if score>0.075:\n",
    "            comments.append(c_id)\n",
    "            comments_body.append(c_body)\n",
    "            comments_author.append(c_author)\n",
    "            \n",
    "    if len(comments)>1000000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.DataFrame({'ID':comments,'body':comments_body,'author':comments_author})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify race identity of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = list(set(database['author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a94f72e57364e3a8f97fd840ffc3357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13921 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys_w=[\"I\\'m a white\",\"I am a white\",\"I\\'m white\",\"I am white\",\"As a white\"]\n",
    "keys_b=[\"I\\'m a black\",\"I am a black\",\"I\\'m black\",\"I am black\",\"As a black\"]\n",
    "race={}\n",
    "for author in tqdm_notebook(author_list):\n",
    "    race[author]=0\n",
    "    for key_w in keys_w:\n",
    "        comments = list(api.search_comments(author=author, q=key_w))\n",
    "        if len(comments)>0:\n",
    "            race[author]+=-1 \n",
    "            break\n",
    "    for key_b in keys_b:\n",
    "        comments = list(api.search_comments(author=author, q=key_b))\n",
    "        if len(comments)>0:\n",
    "            race[author]+=1 \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_label=[]\n",
    "for author in database['author']:\n",
    "    if race[author]==1:\n",
    "        race_label.append('black')\n",
    "    elif race[author]==-1:\n",
    "        race_label.append('white')\n",
    "    else:\n",
    "        race_label.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "database['race']=race_label\n",
    "databas = database.dropna()\n",
    "database.to_csv('data/complete_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>gw34ano</td>\n",
       "      <td>This story is so horrific - but as someone who...</td>\n",
       "      <td>wwh0428</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>gw2zze6</td>\n",
       "      <td>&gt;Rail to Windward in Alpharetta was on the age...</td>\n",
       "      <td>Autolycus25</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>gw2qmfi</td>\n",
       "      <td>Elliott St Pub is across the street from the B...</td>\n",
       "      <td>baegelsandlox</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>gw2g29x</td>\n",
       "      <td>That’s going to be the huge difference. Downto...</td>\n",
       "      <td>Takedown22</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>gw2a3mu</td>\n",
       "      <td>I think that is of the biggest problems I have...</td>\n",
       "      <td>skuhlke</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199450</td>\n",
       "      <td>c07al7c</td>\n",
       "      <td>Terry here.\\r\\n\\r\\nI live in Carrollton. 40 mi...</td>\n",
       "      <td>terrister</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199451</td>\n",
       "      <td>c07a5qh</td>\n",
       "      <td>I live at the intersection of Atlanta, Smyrna,...</td>\n",
       "      <td>rossdub</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199453</td>\n",
       "      <td>c07a1ea</td>\n",
       "      <td>Grew up in East Cobb, went to Ga Tech, then bo...</td>\n",
       "      <td>jodythebad</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199454</td>\n",
       "      <td>c07a091</td>\n",
       "      <td>i'm shane i'm 20 and live in smyrna!</td>\n",
       "      <td>tugteen</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199455</td>\n",
       "      <td>c079v99</td>\n",
       "      <td>Live in Decatur between the square and Emory. ...</td>\n",
       "      <td>corkill</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55685 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               body  \\\n",
       "0       gw34ano  This story is so horrific - but as someone who...   \n",
       "3       gw2zze6  >Rail to Windward in Alpharetta was on the age...   \n",
       "7       gw2qmfi  Elliott St Pub is across the street from the B...   \n",
       "9       gw2g29x  That’s going to be the huge difference. Downto...   \n",
       "15      gw2a3mu  I think that is of the biggest problems I have...   \n",
       "...         ...                                                ...   \n",
       "199450  c07al7c  Terry here.\\r\\n\\r\\nI live in Carrollton. 40 mi...   \n",
       "199451  c07a5qh  I live at the intersection of Atlanta, Smyrna,...   \n",
       "199453  c07a1ea  Grew up in East Cobb, went to Ga Tech, then bo...   \n",
       "199454  c07a091               i'm shane i'm 20 and live in smyrna!   \n",
       "199455  c079v99  Live in Decatur between the square and Emory. ...   \n",
       "\n",
       "               author   race  \n",
       "0             wwh0428   None  \n",
       "3         Autolycus25   None  \n",
       "7       baegelsandlox   None  \n",
       "9          Takedown22   None  \n",
       "15            skuhlke   None  \n",
       "...               ...    ...  \n",
       "199450      terrister   None  \n",
       "199451        rossdub  black  \n",
       "199453     jodythebad   None  \n",
       "199454        tugteen  black  \n",
       "199455        corkill   None  \n",
       "\n",
       "[55685 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = database.drop_duplicates()\n",
    "database.reset_index().drop(columns=[\"index\"])\n",
    "database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
