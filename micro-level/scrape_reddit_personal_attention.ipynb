{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/h/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/h/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/h/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import nltk.data\n",
    "import warnings\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from psaw import PushshiftAPI\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "!pip install google-colab\n",
    "from google.colab import auth\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "auth.authenticate_user()\n",
    "print('Authenticated')\n",
    "client = bigquery.Client(project='socialseg')\n",
    "\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=[]\n",
    "for i in range(2010,2015):\n",
    "    dates.append(i)\n",
    "for i in range(1,13):\n",
    "    for year in range(5,9):\n",
    "        month = '0{}'.format(i)[-2:]\n",
    "        dates.append('201{}_{}'.format(year, month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape raw data from Reddit with Bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_body = []\n",
    "comments_author = []\n",
    "subreddits = []\n",
    "\n",
    "for date in dates:\n",
    "    print(date,len(comments_body))\n",
    "    query = \"\"\"\n",
    "            SELECT body,author,subreddit \n",
    "            FROM `fh-bigquery.reddit_comments.{}` \n",
    "            WHERE body LIKE \"% live %\" and author != '[deleted]'\n",
    "            \"\"\".format(date)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    i = 0\n",
    "    for row in query_job:\n",
    "        comments_body.append(row.body)\n",
    "        comments_author.append(row.author)\n",
    "        subreddits.append(row.subreddit)\n",
    "        i+=1\n",
    "        if i>5500:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv('full_set.csv',index_col=0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.DataFrame({'body':comments_body,'author':comments_author,'subreddit':subreddits})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We live in the future.</td>\n",
       "      <td>Paul-ish</td>\n",
       "      <td>gadgets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What?? You actually censor a word like that fr...</td>\n",
       "      <td>TheBigPanda</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I live here too. I can confirm that Fox 2 sucks!</td>\n",
       "      <td>Braunie</td>\n",
       "      <td>offbeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Use photoshop to put them all together on one ...</td>\n",
       "      <td>quietlight</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Man I would hate to live in Russia. They make ...</td>\n",
       "      <td>DigiSerf</td>\n",
       "      <td>environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291548</th>\n",
       "      <td>Two big things improved my depression. First, ...</td>\n",
       "      <td>cristinewithnoh</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291549</th>\n",
       "      <td>Oh yes, the world works this way, and we certa...</td>\n",
       "      <td>UhOhFeministOnReddit</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291550</th>\n",
       "      <td>UFC 232 live streaming free HD TV \\n</td>\n",
       "      <td>xumaes</td>\n",
       "      <td>UFC231Live4kTv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291551</th>\n",
       "      <td>UFC 232 live streaming free HD TV \\n</td>\n",
       "      <td>xumaes</td>\n",
       "      <td>UFC231Live4kTv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291552</th>\n",
       "      <td>UFC 232 live streaming free HD TV \\n</td>\n",
       "      <td>xumaes</td>\n",
       "      <td>UFC231Live4kTv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291573 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body  \\\n",
       "0                                  We live in the future.   \n",
       "1       What?? You actually censor a word like that fr...   \n",
       "2       I live here too. I can confirm that Fox 2 sucks!    \n",
       "3       Use photoshop to put them all together on one ...   \n",
       "4       Man I would hate to live in Russia. They make ...   \n",
       "...                                                   ...   \n",
       "291548  Two big things improved my depression. First, ...   \n",
       "291549  Oh yes, the world works this way, and we certa...   \n",
       "291550               UFC 232 live streaming free HD TV \\n   \n",
       "291551               UFC 232 live streaming free HD TV \\n   \n",
       "291552               UFC 232 live streaming free HD TV \\n   \n",
       "\n",
       "                      author           subreddit  \n",
       "0                   Paul-ish             gadgets  \n",
       "1                TheBigPanda              sports  \n",
       "2                    Braunie             offbeat  \n",
       "3                 quietlight               apple  \n",
       "4                   DigiSerf         environment  \n",
       "...                      ...                 ...  \n",
       "291548       cristinewithnoh           AskReddit  \n",
       "291549  UhOhFeministOnReddit  BlackPeopleTwitter  \n",
       "291550                xumaes      UFC231Live4kTv  \n",
       "291551                xumaes      UFC231Live4kTv  \n",
       "291552                xumaes      UFC231Live4kTv  \n",
       "\n",
       "[291573 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out similar content data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "def get_consine_sim(*strs):\n",
    "    vectors = [t for t in get_vectors(*strs)]\n",
    "    return cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Examples.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "verified = [line.strip() for line in lines if line!='\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94377aebfe094dcf83750a693bf8f968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=291538.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_comments = []\n",
    "authors = []\n",
    "for i in tqdm_notebook(range(len(database['author']))):\n",
    "    data_one = database.iloc[i]\n",
    "    author = data_one['author']\n",
    "    body = data_one['body']\n",
    "    raw_sentences = tokenizer.tokenize(body.strip())\n",
    "\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)<10:\n",
    "            continue\n",
    "        if re.sub(\"[^a-zA-Z]\", \"\", raw_sentence)==\"\":\n",
    "            continue\n",
    "        score = np.mean([get_consine_sim(raw_sentence,v)[1,0] for v in verified])\n",
    "        if score>0.075:\n",
    "            valid_comments.append(raw_sentence)\n",
    "            authors.append(author)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.DataFrame({'body':valid_comments,'author':authors})\n",
    "df_valid.to_csv('valid_sentences.csv')\n",
    "df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label authors' races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_authors = []\n",
    "\n",
    "for date in dates:\n",
    "    query = \"\"\"\n",
    "            SELECT body,author \n",
    "            FROM `fh-bigquery.reddit_comments.{}` \n",
    "            WHERE (body LIKE \"%I\\'m black %\" OR body LIKE \"%I am black %\" OR body LIKE \"%As a black %\")\n",
    "            \"\"\".format(date)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    for row in query_job:\n",
    "        black_authors.append(row.author)\n",
    "\n",
    "white_authors = []\n",
    "\n",
    "for date in dates:\n",
    "    query = \"\"\"\n",
    "            SELECT body,author \n",
    "            FROM `fh-bigquery.reddit_comments.{}` \n",
    "            WHERE (body LIKE \"%I\\'m white %\" OR body LIKE \"%I am white %\" OR body LIKE \"%As a white %\")\n",
    "            \"\"\".format(date)\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    for row in query_job:\n",
    "        white_authors.append(row.author)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_authors = []\n",
    "with open(\"white.txt\",\"r\") as f:\n",
    "    while 1:\n",
    "        line=f.readline().strip()\n",
    "        if line=='':\n",
    "            break\n",
    "        white_authors.append(line)\n",
    "\n",
    "black_authors = []\n",
    "with open(\"black.txt\",\"r\") as f:\n",
    "    while 1:\n",
    "        line=f.readline().strip()\n",
    "        if line=='':\n",
    "            break\n",
    "        black_authors.append(line)\n",
    "\n",
    "invalid_class = []\n",
    "with open(\"invalid.txt\",\"r\") as f:\n",
    "    while 1:\n",
    "        line=f.readline().strip()\n",
    "        if line=='':\n",
    "            break\n",
    "        invalid_class.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_authors = set(white_authors)\n",
    "black_authors = set(black_authors)\n",
    "invalid_class = white_authors.intersection(black_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_race = []\n",
    "for i in range(len(df_valid)):\n",
    "    author = df_valid.iloc[i]['authors']\n",
    "    if author == '[deleted]':\n",
    "        author_race.append(None)\n",
    "        continue\n",
    "    if author in white_authors and author not in invalid_class:\n",
    "        author_race.append(\"white\")\n",
    "    elif author in black_authors and author not in invalid_class:\n",
    "        author_race.append(\"black\")\n",
    "    else:\n",
    "        author_race.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['race']=author_race\n",
    "df_valid = df_valid.dropna()\n",
    "df_valid = df_valid[['body','race']].reset_index().drop(columns=['index'])\n",
    "df_valid.to_csv('valid_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
