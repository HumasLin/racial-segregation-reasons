{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw\n",
    "!pip install psaw\n",
    "import re\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "nltk.download('stopwords')\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "api = PushshiftAPI()\n",
    "reddit = praw.Reddit(client_id='niNh_Elui1OusA', client_secret='gSMcp4CW3nHLolSLmGkiI6Dq_fw', user_agent='Scraping')\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "\n",
    "    no_cite = re.sub('>.*','',text)\n",
    "\n",
    "    no_url = re.sub(r\"http\\S+\", '', no_cite)\n",
    "\n",
    "    review_text = BeautifulSoup(no_url).get_text()\n",
    "\n",
    "    #Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    #Convert to lower case and split\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    #Covert stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    #Remove stopwords\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    #Join the words back into one string separated by space\n",
    "    clean_text = \" \".join(meaningful_words)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_vectors(*strs):\n",
    "    text = [clean(t) for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "def get_consine_sim(*strs):\n",
    "    vectors = [t for t in get_vectors(*strs)]\n",
    "    return cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape raw data from Reddit with PRAW and PSAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Examples.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "verified = [line.strip() for line in lines if line!='\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen1 = api.search_comments(q=' live ',subreddit='Atlanta')\n",
    "\n",
    "comments = []\n",
    "comments_body = []\n",
    "comments_author = []\n",
    "\n",
    "for i in gen1:\n",
    "\n",
    "    c_id = str(i)\n",
    "    comment_entity=reddit.comment(c_id)\n",
    "    c_body = comment_entity.body\n",
    "    c_author = str(comment_entity.author)\n",
    "    verified_rest = list(set(verified) - set([i]))\n",
    "    score = max([np.mean([get_consine_sim(text,j)[1,0] for j in verified_rest]) for text in c_body.split('\\n')])\n",
    "    \n",
    "    if score>0.075:\n",
    "        comments.append(c_id)\n",
    "        comments_body.append(c_body)\n",
    "        comments_author.append(c_author)\n",
    "    if len(comments)>1000000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.DataFrame({'ID':comments,'body':comments_body,'author':comments_author})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify race identity of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = list(set(database['author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_w=[\"I\\'m a white\",\"I am a white\",\"I\\'m white\",\"I am white\",\"As a white\"]\n",
    "keys_b=[\"I\\'m a black\",\"I am a black\",\"I\\'m black\",\"I am black\",\"As a black\"]\n",
    "race={}\n",
    "for author in author_list:\n",
    "    race[author]=0\n",
    "    for key_w in keys_w:\n",
    "        comments = api.search_comments(author=author, q=key_w)\n",
    "        for comment_id in comments:\n",
    "            comment_body = reddit.comment(str(comment_id))\n",
    "            if comment_body:\n",
    "                race[author]+=-1 \n",
    "    for key_b in keys_b:\n",
    "        comments = api.search_comments(author=author, q=key_w)\n",
    "        for comment_id in comments:\n",
    "            comment_body = reddit.comment(str(comment_id))\n",
    "            if comment_body:\n",
    "                race[author]+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_label=[]\n",
    "for author in database['author']:\n",
    "    if race[author]==1:\n",
    "        race_label.append('black')\n",
    "    elif race[author]==-1:\n",
    "        race_label.append('white')\n",
    "    else:\n",
    "        race_label.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database['race']=race_label\n",
    "databas = database.dropna()\n",
    "database.to_csv('data/complete_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
