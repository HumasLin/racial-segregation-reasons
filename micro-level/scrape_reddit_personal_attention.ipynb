{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/anaconda3/lib/python3.7/site-packages (7.2.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /opt/anaconda3/lib/python3.7/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2 in /opt/anaconda3/lib/python3.7/site-packages (from praw) (2.0.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/lib/python3.7/site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from update-checker>=0.18->praw) (2.22.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (2020.6.20)\n",
      "Requirement already satisfied: psaw in /opt/anaconda3/lib/python3.7/site-packages (0.1.0)\n",
      "Requirement already satisfied: Click in /opt/anaconda3/lib/python3.7/site-packages (from psaw) (7.1.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from psaw) (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->psaw) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->psaw) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->psaw) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->psaw) (2.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/humas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/humas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install praw\n",
    "!pip install psaw\n",
    "import re\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm_notebook\n",
    "nltk.download('stopwords')\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from psaw import PushshiftAPI\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "api = PushshiftAPI()\n",
    "reddit = praw.Reddit(client_id='niNh_Elui1OusA', client_secret='gSMcp4CW3nHLolSLmGkiI6Dq_fw', user_agent='Scraping')\n",
    "# reddit = praw.Reddit(client_id='client_id', client_secret='client_secret', user_agent='user_agent')\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "\n",
    "    no_cite = re.sub('>.*','',text)\n",
    "\n",
    "    no_url = re.sub(r\"http\\S+\", '', no_cite)\n",
    "\n",
    "    review_text = BeautifulSoup(no_url).get_text()\n",
    "\n",
    "    #Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    #Convert to lower case and split\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    #Covert stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    #Remove stopwords\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    #Join the words back into one string separated by space\n",
    "    clean_text = \" \".join(meaningful_words)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_vectors(*strs):\n",
    "    text = [clean(t) for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "def get_consine_sim(*strs):\n",
    "    vectors = [t for t in get_vectors(*strs)]\n",
    "    return cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape raw data from Reddit with PRAW and PSAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Examples.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "verified = [line.strip() for line in lines if line!='\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e58a3be2e94fc6bee7319201fd1b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen1 = api.search_comments(q=' live ',subreddit='Atlanta')\n",
    "\n",
    "comments = []\n",
    "comments_body = []\n",
    "comments_author = []\n",
    "\n",
    "for i in tqdm_notebook(gen1):\n",
    "\n",
    "    c_id = str(i)\n",
    "    comment_entity=reddit.comment(c_id)\n",
    "    c_body = comment_entity.body\n",
    "    c_author = str(comment_entity.author)\n",
    "    verified_rest = list(set(verified) - set([i]))\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(c_body.strip())\n",
    "\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence.split())<5:\n",
    "            continue\n",
    "        if re.sub(\"[^a-zA-Z]\", \"\", raw_sentence)==\"\":\n",
    "            continue\n",
    "        score = max([get_consine_sim(raw_sentence,v)[1,0] for v in verified])\n",
    "        if score>0.075:\n",
    "            comments.append(c_id)\n",
    "            comments_body.append(c_body)\n",
    "            comments_author.append(c_author)\n",
    "            \n",
    "    if len(comments)>1000000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.DataFrame({'ID':comments,'body':comments_body,'author':comments_author})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify race identity of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = list(set(database['author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec46944f6e54ab29c24eaf463addc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys_w=[\"I\\'m a white\",\"I am a white\",\"I\\'m white\",\"I am white\",\"As a white\"]\n",
    "keys_b=[\"I\\'m a black\",\"I am a black\",\"I\\'m black\",\"I am black\",\"As a black\"]\n",
    "race={}\n",
    "for author in tqdm_notebook(author_list):\n",
    "    race[author]=0\n",
    "    for key_w in keys_w:\n",
    "        comments = list(api.search_comments(author=author, q=key_w))\n",
    "        if len(comments)>0:\n",
    "            race[author]+=-1 \n",
    "            break\n",
    "    for key_b in keys_b:\n",
    "        comments = list(api.search_comments(author=author, q=key_b))\n",
    "        if len(comments)>0:\n",
    "            race[author]+=1 \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_label=[]\n",
    "for author in database['author']:\n",
    "    if race[author]==1:\n",
    "        race_label.append('black')\n",
    "    elif race[author]==-1:\n",
    "        race_label.append('white')\n",
    "    else:\n",
    "        race_label.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database['race']=race_label\n",
    "databas = database.dropna()\n",
    "database.to_csv('data/complete_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
