{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/h/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/h/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/h/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "import json\n",
    "import time\n",
    "import spacy \n",
    "import gensim \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim import corpora \n",
    "import collections\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from spacy import tokens\n",
    "from typing import Iterable, List, Set\n",
    "\n",
    "import nltk \n",
    "from nltk import FreqDist \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CMAP_DIR = os.path.join(os.getcwd(), \"data/contractions.txt\")\n",
    "with open(_CMAP_DIR, \"r\") as file:\n",
    "    _CMAP = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words(lexicon_dir: str) -> Set[str]:\n",
    "    file = open(os.path.join(lexicon_dir), encoding=\"ISO-8859-1\")\n",
    "    return set(line.strip() for line in file.readlines())\n",
    "    \n",
    "_LEXICON_DIR = os.path.join(os.getcwd(), \"\")\n",
    "_POS_WORDS = load_words(os.path.join(_LEXICON_DIR, \"data/pos_words.txt\"))\n",
    "_NEG_WORDS = load_words(os.path.join(_LEXICON_DIR, \"data/neg_words.txt\"))\n",
    "_OPINION_WORDS = _POS_WORDS | _NEG_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(cityName):\n",
    "    data = pd.read_csv('data/{}.csv'.format(cityName),lineterminator='\\n')\n",
    "    with_urls = []\n",
    "    idx_urls = []\n",
    "    i = 0\n",
    "    for string in data[\"body\"]:\n",
    "        try:\n",
    "            urls = re.findall(\"(?P<url>https?://[^\\s]+)\", string)\n",
    "            if urls != []:\n",
    "                with_urls.append(string)\n",
    "                idx_urls.append(i)\n",
    "        except:\n",
    "            idx_urls.append(i)\n",
    "        i += 1\n",
    "    \n",
    "    idx_urls = list(set(idx_urls))\n",
    "    data = data.drop(index=idx_urls).reset_index()\n",
    "    data = data.drop(columns=['index','Unnamed: 0'])\n",
    "    return data\n",
    "\n",
    "with open('data/cities.txt','r') as f:\n",
    "    cities = []\n",
    "    while 1:\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        cities.append(line.strip())\n",
    "cities = cities[:-1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=_CMAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(\n",
    "        contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        if contraction_mapping.get(match):\n",
    "            expanded_contraction = contraction_mapping.get(match)\n",
    "        else:\n",
    "            expanded_contraction = contraction_mapping.get(match.lower())\n",
    "        if expanded_contraction!=None:\n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'s\", \"\", expanded_text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    if remove_digits:\n",
    "        pattern = r\"[^.a-zA-z\\s]\"\n",
    "    else:\n",
    "        pattern = r\"[^a-zA-z0-9.!?\\s]\"\n",
    "\n",
    "    # Substitute all special characters with spaces\n",
    "    text = re.sub(pattern, \" \", text)\n",
    "    # Substitute any white space character with a single space\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocessing(texts):\n",
    "    texts = texts.map(expand_contractions)\n",
    "    texts = texts.map(remove_special_characters)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_spacy(texts):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    start_time = time.time()\n",
    "    docs = list(nlp.pipe(texts))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinion mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_opinion_mod(token):\n",
    "    is_mod = token.dep_ in {\"amod\", \"advmod\"}\n",
    "    is_op = token.text.lower() in _OPINION_WORDS\n",
    "    return is_mod and is_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_aspects(docs):\n",
    "\n",
    "    sent_dict_list = []\n",
    "    for doc in docs:\n",
    "        sent_dict = collections.Counter()\n",
    "        for token in doc:\n",
    "      # check if the word is an opinion word, then assign sentiment\n",
    "            if token.text.lower() in _OPINION_WORDS:\n",
    "                if token.text.lower() in _POS_WORDS:\n",
    "                    sentiment = 1 \n",
    "                elif token.text.lower() in _NEG_WORDS:\n",
    "                    sentiment = -1\n",
    "                else:\n",
    "                    sentiment = 0\n",
    "                    \n",
    "                if (token.dep_ == \"advmod\"):\n",
    "                    continue\n",
    "                elif (token.dep_ == \"amod\"):\n",
    "                    sent_dict[token.head.text.lower()] += sentiment\n",
    "                else:\n",
    "                    for child in token.children:\n",
    "                        if _is_opinion_mod(child):\n",
    "                            sentiment *= 1.5\n",
    "                        # check for negation words and flip the sign of sentiment\n",
    "                        if child.dep_ == \"neg\":\n",
    "                            sentiment *= -1\n",
    "                    for child in token.children:\n",
    "                        if (token.pos_ == \"VERB\") & (child.dep_ == \"dobj\"):\n",
    "                            # if verb, check if there's a direct object\n",
    "                            sent_dict[child.text.lower()] += sentiment\n",
    "                            # check for conjugates (a AND b), then add both to dictionary\n",
    "                            subchildren = []\n",
    "                            conj = 0\n",
    "                            for subchild in child.children:\n",
    "                                if subchild.text.lower() == \"and\": \n",
    "                                    conj=1\n",
    "                                if (conj == 1) and (subchild.text.lower() != \"and\"):\n",
    "                                    subchildren.append(subchild.text.lower())\n",
    "                                    conj = 0\n",
    "                            for subchild in subchildren:\n",
    "                                sent_dict[subchild] += sentiment              \n",
    "                    # check for negation\n",
    "                    for child in token.head.children:\n",
    "                        noun = \"\"\n",
    "                        if _is_opinion_mod(child):\n",
    "                            sentiment *= 1.5\n",
    "                        if (child.dep_ == \"neg\"):\n",
    "                            sentiment *= -1\n",
    "\n",
    "                    # check for nouns\n",
    "                    for child in token.head.children:\n",
    "                        noun = \"\"\n",
    "                        if (child.pos_ == \"NOUN\") and (child.text not in sent_dict):\n",
    "                            noun = child.text.lower()\n",
    "                            # Check for compound nouns\n",
    "                            for subchild in child.children:\n",
    "                                if subchild.dep_ == \"compound\":\n",
    "                                    noun = subchild.text.lower() + \" \" + noun\n",
    "                                    sent_dict[noun] += sentiment\n",
    "        sent_dict_list.append(collections.Counter(sent_dict))\n",
    "\n",
    "    return sent_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_aspects(csv_path: str):\n",
    "    reviews = data_prepare(csv_path)\n",
    "\n",
    "    reviews = reviews['body'][:50000]\n",
    "    n_reviews = len(reviews)\n",
    "    print(\"Collected {} comments from city {}\".format(n_reviews,csv_path))\n",
    "    if n_reviews < 5000:\n",
    "        return []\n",
    "\n",
    "    valid_reviews = reviews.dropna()\n",
    "    n_reviews = len(valid_reviews)\n",
    "\n",
    "    # Basic preprocessing\n",
    "    texts = basic_preprocessing(valid_reviews)\n",
    "\n",
    "    # Create spacy docs using `nlp.pipe`\n",
    "    spacy_docs = apply_spacy(texts)\n",
    "    # Use docs to find aspects\n",
    "    aspects = sentiment_aspects(spacy_docs)\n",
    "\n",
    "    # Add columns to the DataFrame\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    valid_reviews[\"processed_text\"] = texts\n",
    "    valid_reviews[\"aspects\"] = aspects\n",
    "\n",
    "    return valid_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 50000 comments from city NYC\n",
      "Get NYC aspects from 50002\n",
      "\n",
      "607.9321100711823\n",
      "Collected 16132 comments from city Los Angeles\n",
      "Get Los Angeles aspects from 16134\n",
      "\n",
      "252.4024670124054\n",
      "Collected 50000 comments from city Chicago\n",
      "Get Chicago aspects from 50002\n",
      "\n",
      "613.0907161235809\n",
      "Collected 35552 comments from city Dallas\n",
      "Get Dallas aspects from 35554\n",
      "\n",
      "323.6276578903198\n",
      "Collected 50000 comments from city Houston\n",
      "Get Houston aspects from 50002\n",
      "\n",
      "409.7468559741974\n",
      "Collected 6555 comments from city Washington\n",
      "Get Washington aspects from 6557\n",
      "\n",
      "73.87604904174805\n",
      "Collected 20526 comments from city Miami\n",
      "Get Miami aspects from 20528\n",
      "\n",
      "175.09253478050232\n",
      "Collected 20425 comments from city Philadelphia\n",
      "Get Philadelphia aspects from 20427\n",
      "\n",
      "197.79173231124878\n",
      "Collected 50000 comments from city Atlanta\n",
      "Get Atlanta aspects from 50002\n",
      "\n",
      "430.67121410369873\n",
      "Collected 20512 comments from city Phoenix\n",
      "Get Phoenix aspects from 20514\n",
      "\n",
      "192.23252701759338\n",
      "Collected 50000 comments from city Boston\n",
      "Get Boston aspects from 50002\n",
      "\n",
      "472.4856240749359\n",
      "Collected 19508 comments from city San Francisco\n",
      "Get San Francisco aspects from 19510\n",
      "\n",
      "242.50846695899963\n",
      "Collected 145 comments from city Riverside\n",
      "Collected 30194 comments from city Detroit\n",
      "Get Detroit aspects from 30196\n",
      "\n",
      "310.06008791923523\n",
      "Collected 50000 comments from city Seattle\n",
      "Get Seattle aspects from 50002\n",
      "\n",
      "486.0937569141388\n",
      "Collected 12968 comments from city Minneapolis\n",
      "Get Minneapolis aspects from 12970\n",
      "\n",
      "142.1048538684845\n",
      "Collected 27591 comments from city San Diego\n",
      "Get San Diego aspects from 27593\n",
      "\n",
      "289.7006058692932\n",
      "Collected 18308 comments from city Tampa\n",
      "Get Tampa aspects from 18310\n",
      "\n",
      "153.629980802536\n",
      "Collected 50000 comments from city Denver\n",
      "Get Denver aspects from 50002\n",
      "\n",
      "463.0104949474335\n",
      "Collected 16903 comments from city St. Louis\n",
      "Get St. Louis aspects from 16905\n",
      "\n",
      "196.02279210090637\n",
      "Collected 39195 comments from city Baltimore\n",
      "Get Baltimore aspects from 39197\n",
      "\n",
      "408.79397678375244\n",
      "Collected 22366 comments from city Charlotte\n",
      "Get Charlotte aspects from 22368\n",
      "\n",
      "206.1768569946289\n",
      "Collected 16757 comments from city Orlando\n",
      "Get Orlando aspects from 16759\n",
      "\n",
      "143.04490208625793\n",
      "Collected 9658 comments from city San Antonio\n",
      "Get San Antonio aspects from 9660\n",
      "\n",
      "96.37142705917358\n",
      "Collected 50000 comments from city Portland\n",
      "Get Portland aspects from 50002\n",
      "\n",
      "451.7695622444153\n",
      "Collected 16412 comments from city Sacramento\n",
      "Get Sacramento aspects from 16414\n",
      "\n",
      "169.3447139263153\n",
      "Collected 35936 comments from city Pittsburgh\n",
      "Get Pittsburgh aspects from 35938\n",
      "\n",
      "330.86706495285034\n",
      "Collected 1216 comments from city Las Vegas\n",
      "Collected 50000 comments from city Austin\n",
      "Get Austin aspects from 50002\n",
      "\n",
      "427.1000452041626\n",
      "Collected 17107 comments from city Cincinnati\n",
      "Get Cincinnati aspects from 17109\n",
      "\n",
      "173.9528489112854\n",
      "Collected 6609 comments from city Kansas City\n",
      "Get Kansas City aspects from 6611\n",
      "\n",
      "77.59921216964722\n",
      "Collected 28381 comments from city Columbus\n",
      "Get Columbus aspects from 28383\n",
      "\n",
      "251.4498360157013\n",
      "Collected 4482 comments from city Indianapolis\n",
      "Collected 17940 comments from city Cleveland\n",
      "Get Cleveland aspects from 17942\n",
      "\n",
      "166.83563804626465\n",
      "Collected 10064 comments from city San Jose\n",
      "Get San Jose aspects from 10066\n",
      "\n",
      "105.08942604064941\n",
      "Collected 33001 comments from city Nashville\n",
      "Get Nashville aspects from 33003\n",
      "\n",
      "319.6676301956177\n",
      "Collected 401 comments from city Virginia Beach\n",
      "Collected 4402 comments from city Providence\n",
      "Collected 13790 comments from city Milwaukee\n",
      "Get Milwaukee aspects from 13792\n",
      "\n",
      "128.00923800468445\n",
      "Collected 4442 comments from city Jacksonville\n",
      "Collected 5 comments from city Oklahoma City\n",
      "Collected 18395 comments from city Raleigh\n",
      "Get Raleigh aspects from 18397\n",
      "\n",
      "160.94051814079285\n",
      "Collected 0 comments from city Richmond\n",
      "Collected 13408 comments from city New Orleans\n",
      "Get New Orleans aspects from 13410\n",
      "\n",
      "144.57205605506897\n",
      "Collected 13579 comments from city Louisville\n",
      "Get Louisville aspects from 13581\n",
      "\n",
      "132.75220584869385\n",
      "Collected 6501 comments from city Salt Lake\n",
      "Get Salt Lake aspects from 6503\n",
      "\n",
      "76.01174473762512\n",
      "Collected 1005 comments from city Hartford\n",
      "Collected 17601 comments from city Buffalo\n",
      "Get Buffalo aspects from 17603\n",
      "\n",
      "180.52383828163147\n",
      "Collected 11823 comments from city Birmingham\n",
      "Get Birmingham aspects from 11825\n",
      "\n",
      "125.07152700424194\n"
     ]
    }
   ],
   "source": [
    "city_aspects = {}\n",
    "for city in cities:\n",
    "    start_time = time.time()\n",
    "    aspect = find_aspects(city)\n",
    "    if len(aspect) == 0:\n",
    "        continue\n",
    "    print(\"Get {} aspects from {}\\n\".format(city, len(aspect)))\n",
    "    print(time.time()-start_time)\n",
    "\n",
    "    aspect_dict = {}\n",
    "    aspect_refine = {}\n",
    "    for i in aspect['aspects']:\n",
    "        items=list(i.items())\n",
    "        for item in items:\n",
    "            try:\n",
    "                aspect_dict[item[0]].append(item[1])\n",
    "            except:\n",
    "                aspect_dict[item[0]] = []\n",
    "    for term in aspect_dict.keys():\n",
    "        if len(aspect_dict[term])>10:\n",
    "            aspect_refine[term]=np.mean(aspect_dict[term])\n",
    "    city_aspects[city] = aspect_refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/city_aspects.npy',city_aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_city = []\n",
    "feature_set = []\n",
    "for city in city_aspects:\n",
    "    if len(city_aspects[city].keys())>50:\n",
    "        valid_city.append(city)\n",
    "        feature_set = feature_set + list(city_aspects[city].keys())\n",
    "feature_set = set(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/features.txt\",\"w\") as f:\n",
    "    for feature in feature_set:\n",
    "        f.write(feature+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_feature_final = pd.read_csv('data/keyword_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_keys = {}\n",
    "for i in list(dict(zip(keyword_feature_final['category'],keyword_feature_final['keywords'])).items()):\n",
    "    feature_keys[i[0][1:-1]]=[k[1:-1] for k in i[1][1:-1].split(\",\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create city data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_value = {\"city\":[]}\n",
    "for city in city_aspects:\n",
    "    feature_value[\"city\"].append(city)\n",
    "    aspect_city = city_aspects[city]\n",
    "    for feature in feature_keys:\n",
    "        key_list = feature_keys[feature]\n",
    "        key_value = []\n",
    "        for key in key_list:\n",
    "            try:\n",
    "                key_value.append(aspect_city[key])\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            value = np.mean(key_value)\n",
    "        except:\n",
    "            value = 0\n",
    "        try:\n",
    "            feature_value[feature].append(value)\n",
    "        except:\n",
    "            feature_value[feature] = [value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(feature_value)\n",
    "df_final.to_csv('data/before_dropna.csv')\n",
    "df_final = df_final.dropna().reset_index().drop(columns=['index'])\n",
    "df_final.to_csv(\"data/aspect_cities.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
